{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # Laboratorium 11 - Spadek wzdłuż gradientu\n"
      ],
      "id": "a4d9d1ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "from scipy.optimize import golden\n",
        "from itertools import cycle"
      ],
      "id": "9f4d6180",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zadanie 2.\n",
        "\n",
        "Należy wyznaczyć najkrótszą ścieżkę robota pomiędzy dwoma punktami $x^{(0)}$ i $x^{(n)}$. Problemem są przeszkody usytuowane na trasie robota, których należy unikać. Zadanie polega na minimalizacji funkcji kosztu, która sprowadza problem nieliniowej optymalizacji z ograniczeniami do problemu nieograniczonego optymalizacji.\n",
        "\n",
        "Macierz $X \\in \\mathbb{R}^{(n+1) \\times 2}$ opisuje ścieżkę złożoną z $n+1$ punktów $x^{(0)}, x^{(1)}, x^{(2)}, \\ldots, x^{(n)}$. Każdy punkt $x^{(i)} \\in \\mathbb{R}^2$. Punkty początkowy i końcowy ścieżki, $x^{(0)}$ i $x^{(n)}$, są ustalone.\n",
        "\n",
        "Punkty z przeszkodami (punkty o 2 współrzędnych), $r^{(j)}$ dane są w macierzy przeszkód $R \\in \\mathbb{R}^{k \\times 2}$.\n",
        "\n",
        "W celu optymalizacji ścieżki robota należy użyć metody największego spadku. Funkcja celu użyta do optymalizacji $F(x^{(0)}, x^{(1)}, \\ldots, x^{(n)})$ zdefiniowana jest jako:\n",
        "\n",
        "$$F(x^{(0)}, x^{(1)}, \\ldots, x^{(n)}) = \\lambda_1 \\sum_{i=0}^{n} \\sum_{j=1}^{k} \\frac{1}{\\epsilon + \\|x^{(i)} - r^{(j)}\\|^2} + \\lambda_2 \\sum_{i=0}^{n-1} \\|x^{(i+1)} - x^{(i)}\\|^2$$\n",
        "\n",
        "Symbole użyte we wzorze mają następujące znaczenie:\n",
        "\n",
        "* Stałe $\\lambda_1$ i $\\lambda_2$ określają wpływ każdego członu wyrażenia na wartość $F(X)$.\n",
        "    * $\\lambda_1$ określa wagę składnika zapobiegającego zbytniemu zbliżaniu się do przeszkody\n",
        "    * $\\lambda_2$ określa wagę składnika zapobiegającego tworzeniu bardzo długich ścieżek\n",
        "\n",
        "* $n$ jest liczbą odcinków, a $n+1$ liczbą punktów na trasie robota.\n",
        "* $k$ jest liczbą przeszkód, których robot musi unikać.\n",
        "* Dodanie $\\epsilon$ w mianowniku zapobiega dzieleniu przez zero.\n",
        "\n",
        "1.  Wyprowadź wyrażenie na gradient $\\nabla F$ funkcji celu $F$ względem $x^{(i)}$:\n",
        "    $\\nabla F = \\left[ \\frac{\\partial F}{\\partial x^{(0)}}, \\ldots, \\frac{\\partial F}{\\partial x^{(n)}} \\right]$.\n",
        "    Wzór wyraź poprzez wektory $x^{(i)}$ i ich składowe, wektory $r^{(j)}$ i ich składowe, $\\epsilon, \\lambda_1, \\lambda_2, n$ i $k$ (niekoniecznie wszystkie).\n",
        "    Wskazówka. $\\frac{\\partial \\|z\\|^2}{\\partial z} = 2z$.\n",
        "\n",
        "2.  Opisz matematycznie i zaimplementuj kroki algorytmu największego spadku z przeszukiwaniem liniowym, który służy do minimalizacji funkcji celu $F$. Do przeszukiwania liniowego (ang. *line search*) użyj metody złotego podziału (ang. *golden section search*). W tym celu załóż, że $F$ jest unimodalna (w rzeczywistości tak nie jest) i że można ustalić początkowy przedział, w którym znajduje się minimum.\n",
        "\n",
        "3.  Znajdź najkrótszą ścieżkę robota przy użyciu algorytmu zaimplementowanego w w poprzednim punkcie. Przyjmij następujące wartości parametrów:\n",
        "    * $n = 20, k = 50$\n",
        "    * $x^{(0)} = [0, 0]$, $x^{(n)} = [20, 20]$\n",
        "    * $r^{(j)} \\sim \\mathcal{U}(0, 20) \\times \\mathcal{U}(0, 20)$\n",
        "    * $\\lambda_1 = \\lambda_2 = 1$\n",
        "    * $\\epsilon = 10^{-13}$\n",
        "    * liczba iteracji = 400\n",
        "\n",
        "Ponieważ nie chcemy zmieniać położenia punktu początkowego i końcowego, $x^{(0)}, x^{(n)}$, wyzeruj gradient funkcji $F$ względem tych punktów.\n",
        "\n",
        "Obliczenia przeprowadź dla 5 różnych losowych inicjalizacji punktów wewnątrz ścieżki $x^{(1)}, \\ldots, x^{(n-1)}$.\n",
        "\n",
        "Narysuj przykładowy wykres wartości funkcji $F$ w zależności od iteracji.\n",
        "\n",
        "Zapewnij powtarzalność wyników, ustawiając wartość odpowiedniego ziarna.\n",
        "\n",
        "![signal-2025-06-17-152231_002.jpeg](attachment:signal-2025-06-17-152231_002.jpeg)\n",
        "\n",
        "Po znalezieniu gradientu funkcji kosztu zaimplementujemy algorytm największego spadku:\n",
        " - inicjalizujemy punkty ścieżki i przeszkody\n",
        " - Powatarzamy poniższe kroki ustaloną liczbę razy (w naszym wypadku 400):\n",
        "   - Obliczamy gradient $\\nabla F$ w punkcie\n",
        "   - Wyznaczamy optymalną wartość kroku za pomocą metody złotego podziału\n",
        "   - Aktualizujemy punkty ścieżki\n",
        "\n",
        "\n",
        "Metoda złotego podziału opiera się na wykorzystaniu szczególnych własności funkcji unimodalnej, to jest takiej, która na danym przedziale posiada dokładnie jedno minimum.\n",
        "\n",
        "Ustalamy współczynnik $t$, $0<t<1$, a następnie powtarzamy poniższe operacje aż do uzyskania żądanej zbieżności:\n",
        " - Obliczamy długość przedziałów $d =t(b-a)$,\n",
        " - Przyjmujemy punkty $l = b-d$, $r=a+d$,\n",
        " - Porównujemy wartości funkcji i decydujemy, czy minimum znajduje się w przedziale lewym, czy prawym i ustawiamy odpowiednio $a$ i $b$.\n"
      ],
      "id": "2256efb6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "n = 20\n",
        "k = 50\n",
        "x_0 = [0, 0]\n",
        "x_n = [20, 20]\n",
        "R = np.random.uniform(0, 20, (k, 2))\n",
        "l_1 = l_2 = 1.0\n",
        "eps = 10e-13\n",
        "iterations = 400"
      ],
      "id": "a766b020",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def cost(X):\n",
        "    A = 0\n",
        "    for i in range(n + 1):\n",
        "        for j in range(k):\n",
        "            A += 1 / (eps + np.linalg.norm(X[i] - R[j]) ** 2)\n",
        "\n",
        "    B = 0\n",
        "    for i in range(n):\n",
        "        B += np.linalg.norm(X[i + 1] - X[i]) ** 2\n",
        "\n",
        "    return l_1 * A + l_2 * B"
      ],
      "id": "f694855c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def grad_cost(X):\n",
        "    gradient = np.zeros_like(X)\n",
        "\n",
        "    for i in range(1, n):\n",
        "        for j in range(k):\n",
        "            diff = X[i] - R[j]\n",
        "            norm_sq = np.linalg.norm(diff) ** 2\n",
        "            gradient[i] += -2 * l_1 * diff / (eps + norm_sq) ** 2\n",
        "\n",
        "        gradient[i] += -2 * l_2 * (X[i + 1] - X[i])\n",
        "        gradient[i] += 2 * l_2 * (X[i] - X[i - 1])\n",
        "\n",
        "    gradient[0] = 0\n",
        "    gradient[-1] = 0\n",
        "\n",
        "    return gradient"
      ],
      "id": "0585ab5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def line_search(X):\n",
        "    costs = []\n",
        "    copyX = copy.deepcopy(X)\n",
        "\n",
        "    def helper(xs, grad):\n",
        "        return lambda alpha: cost(xs + alpha * grad)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        costs.append(cost(copyX))\n",
        "        grad = grad_cost(copyX)\n",
        "\n",
        "        alpha = golden(helper(copyX, -grad))\n",
        "\n",
        "        copyX += alpha * (-grad)\n",
        "\n",
        "        if i > 0 and abs(costs[-1] - costs[-2]) < eps:\n",
        "            break\n",
        "\n",
        "    return (copyX, costs)"
      ],
      "id": "a9100ce8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "XS = []\n",
        "costs = []\n",
        "results = []\n",
        "\n",
        "for i in range(5):\n",
        "    X = np.vstack((x_0, np.random.uniform(0, 20, (n - 1, 2)), x_n))\n",
        "\n",
        "    XS.append(X)\n",
        "\n",
        "    (result, res_cost) = line_search(X)\n",
        "    results.append(result)\n",
        "    costs.append(res_cost)\n",
        "\n",
        "    next_seed = np.random.randint(0, 1000)\n",
        "    np.random.seed(next_seed)"
      ],
      "id": "797ea6ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Koszty końcowe dla każdej próby:\")\n",
        "for i, cost_history in enumerate(costs):\n",
        "    print(f\"Próba {i + 1}: {cost_history[-1]:.6f}\")"
      ],
      "id": "275c13ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Znalezione przez algorytm optymalne ścieżki ilustrują poniższe wykresy:\n"
      ],
      "id": "101d3f40"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_robot_paths():\n",
        "    \"\"\"Rysuje ścieżki robota z wszystkich 5 prób\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    axes_flat = axes.flatten()\n",
        "\n",
        "    for i in range(5):\n",
        "        ax = axes_flat[i]\n",
        "\n",
        "        ax.scatter(\n",
        "            R[:, 0], R[:, 1], c=\"red\", s=30, alpha=0.7, marker=\"x\", label=\"Przeszkody\"\n",
        "        )\n",
        "\n",
        "        initial_path = XS[i]\n",
        "        ax.plot(\n",
        "            initial_path[:, 0],\n",
        "            initial_path[:, 1],\n",
        "            \"b--\",\n",
        "            alpha=0.5,\n",
        "            linewidth=2,\n",
        "            label=\"Ścieżka początkowa\",\n",
        "        )\n",
        "        ax.scatter(initial_path[:, 0], initial_path[:, 1], c=\"blue\", s=20, alpha=0.5)\n",
        "\n",
        "        optimized_path = results[i]\n",
        "        ax.plot(\n",
        "            optimized_path[:, 0],\n",
        "            optimized_path[:, 1],\n",
        "            \"g-\",\n",
        "            linewidth=3,\n",
        "            label=\"Ścieżka zoptymalizowana\",\n",
        "        )\n",
        "        ax.scatter(optimized_path[:, 0], optimized_path[:, 1], c=\"green\", s=30)\n",
        "\n",
        "        ax.scatter(\n",
        "            *x_0,\n",
        "            c=\"black\",\n",
        "            s=150,\n",
        "            marker=\"s\",\n",
        "            label=\"Start\",\n",
        "            edgecolors=\"white\",\n",
        "            linewidth=2,\n",
        "        )\n",
        "        ax.scatter(\n",
        "            *x_n,\n",
        "            c=\"black\",\n",
        "            s=150,\n",
        "            marker=\"^\",\n",
        "            label=\"Koniec\",\n",
        "            edgecolors=\"white\",\n",
        "            linewidth=2,\n",
        "        )\n",
        "\n",
        "        ax.set_xlim(-1, 21)\n",
        "        ax.set_ylim(-1, 21)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.set_title(f\"Próba {i + 1} - Koszt końcowy: {costs[i][-1]:.2f}\")\n",
        "        ax.set_xlabel(\"X\")\n",
        "        ax.set_ylabel(\"Y\")\n",
        "\n",
        "    ax = axes_flat[5]\n",
        "    for i, cost_history in enumerate(costs):\n",
        "        ax.plot(cost_history, label=f\"Próba {i + 1}\", linewidth=2)\n",
        "\n",
        "    ax.set_xlabel(\"Iteracja\")\n",
        "    ax.set_ylabel(\"Wartość funkcji kosztu\")\n",
        "    ax.set_title(\"Historia zbieżności\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax.set_xscale(\"log\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_robot_paths()"
      ],
      "id": "a421bc5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_best_result():\n",
        "    \"\"\"Rysuje najlepszy wynik z większymi szczegółami\"\"\"\n",
        "\n",
        "    final_costs = [cost_history[-1] for cost_history in costs]\n",
        "    best_idx = np.argmin(final_costs)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    ax1.scatter(\n",
        "        R[:, 0], R[:, 1], c=\"red\", s=50, alpha=0.7, marker=\"x\", label=\"Przeszkody\"\n",
        "    )\n",
        "\n",
        "    initial_path = XS[best_idx]\n",
        "    ax1.plot(\n",
        "        initial_path[:, 0],\n",
        "        initial_path[:, 1],\n",
        "        \"b--\",\n",
        "        alpha=0.5,\n",
        "        linewidth=2,\n",
        "        label=\"Ścieżka początkowa\",\n",
        "    )\n",
        "\n",
        "    best_path = results[best_idx]\n",
        "    ax1.plot(\n",
        "        best_path[:, 0], best_path[:, 1], \"g-\", linewidth=4, label=\"Najlepsza ścieżka\"\n",
        "    )\n",
        "    ax1.scatter(\n",
        "        best_path[:, 0],\n",
        "        best_path[:, 1],\n",
        "        c=\"green\",\n",
        "        s=50,\n",
        "        edgecolors=\"black\",\n",
        "        linewidth=1,\n",
        "    )\n",
        "\n",
        "    for i, point in enumerate(best_path):\n",
        "        ax1.annotate(\n",
        "            str(i),\n",
        "            (point[0], point[1]),\n",
        "            xytext=(5, 5),\n",
        "            textcoords=\"offset points\",\n",
        "            fontsize=10,\n",
        "            fontweight=\"bold\",\n",
        "        )\n",
        "\n",
        "    ax1.scatter(\n",
        "        *x_0,\n",
        "        c=\"blue\",\n",
        "        s=200,\n",
        "        marker=\"s\",\n",
        "        label=\"Start\",\n",
        "        edgecolors=\"white\",\n",
        "        linewidth=3,\n",
        "    )\n",
        "    ax1.scatter(\n",
        "        *x_n,\n",
        "        c=\"red\",\n",
        "        s=200,\n",
        "        marker=\"^\",\n",
        "        label=\"Koniec\",\n",
        "        edgecolors=\"white\",\n",
        "        linewidth=3,\n",
        "    )\n",
        "\n",
        "    ax1.set_xlim(-1, 21)\n",
        "    ax1.set_ylim(-1, 21)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\n",
        "        f\"Najlepsza ścieżka (Próba {best_idx + 1})\\nKoszt końcowy: {final_costs[best_idx]:.4f}\"\n",
        "    )\n",
        "    ax1.set_xlabel(\"X\")\n",
        "    ax1.set_ylabel(\"Y\")\n",
        "\n",
        "    ax2.loglog(costs[best_idx], \"g-\", linewidth=3)\n",
        "    ax2.set_xlabel(\"Iteracja\")\n",
        "    ax2.set_ylabel(\"Wartość funkcji kosztu\")\n",
        "    ax2.set_title(f\"Zbieżność najlepszego wyniku\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_yscale(\"log\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Najlepszy wynik z próby {best_idx + 1}\")\n",
        "    print(f\"Koszt końcowy: {final_costs[best_idx]:.6f}\")\n",
        "    print(f\"Liczba iteracji: {len(costs[best_idx])}\")\n",
        "\n",
        "\n",
        "plot_best_result()"
      ],
      "id": "e2a4fe2e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "amc_data_jupyter",
      "language": "python",
      "display_name": "AMC data jupyter",
      "path": "/Users/depebul/Library/Jupyter/kernels/amc_data_jupyter"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}